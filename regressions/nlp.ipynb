{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "consistent-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "subtle-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "animal-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "characteristic-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=brown.categories()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "consecutive-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=brown.sents(categories=\"fiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "thrown-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']]\n"
     ]
    }
   ],
   "source": [
    "# ' '.join(s[4])\n",
    "print(s[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "designing-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenisation\n",
    "# ' '.join(s[4])\n",
    "# s[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "average-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=\"\"\"It was a very pleasent day. The weather was cool and there were light showers. I went to the market to buy some fruits.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "respected-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "dirty-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in s[3:7]:\n",
    "#     document+=' '.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "thick-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-austin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "following-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "paperback-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=sent_tokenize(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "stretch-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasent day.',\n",
       " 'The weather was cool and there were light showers.',\n",
       " 'I went to the market to buy some fruits.']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "lesbian-tanzania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "closed-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =\"send all the 50 docs of 1,2,3 chapters to f43@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "alive-teens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'docs',\n",
       " 'of',\n",
       " '1,2,3',\n",
       " 'chapters',\n",
       " 'to',\n",
       " 'f43@gmail.com']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "tight-microwave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'docs',\n",
       " 'of',\n",
       " '1,2,3',\n",
       " 'chapters',\n",
       " 'to',\n",
       " 'f43',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-creativity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "egyptian-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "##STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "behind-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "final-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "later-bullet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "manufactured-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(text,sw):\n",
    "    useful=[w for w in text if w not in sw]\n",
    "    return useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-privilege",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "naked-brook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'pleasent', 'day.']\n"
     ]
    }
   ],
   "source": [
    "print(remove_sw(sents[0].split(),sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fitted-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REGEX TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "closing-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "leading-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "experienced-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "young-annotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'all', 'the', 'docs', 'of', 'chapters', 'to', 'f', '@gmail.com']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text#HERE WE CAN SEE THAT NUMBERS ARE NOT THERE AND SPECIAL CHARS ARE ALSO NOT\n",
    "#THERE BUT AS WE INCLUDED . AND @ EMAIL IS PRESERVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEMMING and LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "orange-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "variable-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "tested-split",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drink'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('drinking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "worse-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "otherwise-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "passive-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer as wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "amazing-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn=wl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "thick-boulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parlent'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "periodic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BUILDING A VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "passing-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "helpful-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr=(brown.sents(categories='reviews')[52:56])\n",
    "hf=(brown.sents(categories='fiction')[52:56])\n",
    "hn=(brown.sents(categories='news')[52:56])\n",
    "hs=(brown.sents(categories='science_fiction')[52:56])\n",
    "\n",
    "t=[hr,hf,hn,hs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "challenging-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "front-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in t:\n",
    "    sr=\"\"\n",
    "    for j in i:\n",
    "        sr+=(' '.join(j))\n",
    "    corpus.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "catholic-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"An exhibition of Evelyn Cibula's paintings will open with a reception Nov. 5 at the Evanston Community center , 828 Davis St. .It will continue all month .Abstractions and semi-abstractions by Everett McNear are being exhibited by the University Gallery of Notre Dame until Nov. 5 .In the line of operatic trades to cushion the budget , the Dallas Civic Opera will use San Francisco's new Leni Bauer-Ecsy production of `` Lucia Di Lammermoor '' this season , returning the favor next season when San Francisco uses the Dallas `` Don Giovanni '' , designed by Franco Zeffirelli .\",\n",
       " \"`` You don't eat enough , honey .Try to get that down '' .Rachel , observing , would say , `` He has to rediscover his own capacity .It'll take time '' .\",\n",
       " \"The Republicans must hold a primary under the county unit system -- a system which the party opposes in its platform .Sam Caldwell , State Highway Department public relations director , resigned Tuesday to work for Lt. Gov. Garland Byrd's campaign .Caldwell's resignation had been expected for some time .He will be succeeded by Rob Ledford of Gainesville , who has been an assistant more than three years .\",\n",
       " \"Mike liked having light up through the ripples ; ;it was a goodness , beauty .They picnicked by the pool , then lay back on the grass and looked at stars .`` Mike , there's Mars .\"]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-dividend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "accredited-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "creative-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "effective-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "worth-layer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "        2, 0, 4, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 3, 1,\n",
       "        0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 0, 3, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc#HERE WE SEE VOCAB IS 159 length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "streaming-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=vc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "utility-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       2, 0, 4, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 3, 1,\n",
       "       0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "divided-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 3,\n",
       " 'exhibition': 43,\n",
       " 'of': 90,\n",
       " 'evelyn': 40,\n",
       " 'cibula': 21,\n",
       " 'paintings': 97,\n",
       " 'will': 152,\n",
       " 'open': 92,\n",
       " 'with': 153,\n",
       " 'reception': 106,\n",
       " 'nov': 88,\n",
       " 'at': 7,\n",
       " 'the': 129,\n",
       " 'evanston': 39,\n",
       " 'community': 23,\n",
       " 'center': 20,\n",
       " '828': 0,\n",
       " 'davis': 29,\n",
       " 'st': 121,\n",
       " 'it': 66,\n",
       " 'continue': 24,\n",
       " 'all': 2,\n",
       " 'month': 82,\n",
       " 'abstractions': 1,\n",
       " 'and': 4,\n",
       " 'semi': 119,\n",
       " 'by': 15,\n",
       " 'everett': 41,\n",
       " 'mcnear': 80,\n",
       " 'are': 5,\n",
       " 'being': 13,\n",
       " 'exhibited': 42,\n",
       " 'university': 143,\n",
       " 'gallery': 50,\n",
       " 'notre': 87,\n",
       " 'dame': 28,\n",
       " 'until': 144,\n",
       " 'in': 65,\n",
       " 'line': 74,\n",
       " 'operatic': 94,\n",
       " 'trades': 138,\n",
       " 'to': 137,\n",
       " 'cushion': 26,\n",
       " 'budget': 14,\n",
       " 'dallas': 27,\n",
       " 'civic': 22,\n",
       " 'opera': 93,\n",
       " 'use': 146,\n",
       " 'san': 116,\n",
       " 'francisco': 47,\n",
       " 'new': 85,\n",
       " 'leni': 71,\n",
       " 'bauer': 9,\n",
       " 'ecsy': 37,\n",
       " 'production': 103,\n",
       " 'lucia': 78,\n",
       " 'di': 32,\n",
       " 'lammermoor': 68,\n",
       " 'this': 133,\n",
       " 'season': 118,\n",
       " 'returning': 112,\n",
       " 'favor': 45,\n",
       " 'next': 86,\n",
       " 'when': 149,\n",
       " 'uses': 147,\n",
       " 'don': 34,\n",
       " 'giovanni': 53,\n",
       " 'designed': 31,\n",
       " 'franco': 48,\n",
       " 'zeffirelli': 158,\n",
       " 'you': 157,\n",
       " 'eat': 36,\n",
       " 'enough': 38,\n",
       " 'honey': 64,\n",
       " 'try': 139,\n",
       " 'get': 52,\n",
       " 'that': 128,\n",
       " 'down': 35,\n",
       " 'rachel': 105,\n",
       " 'observing': 89,\n",
       " 'would': 155,\n",
       " 'say': 117,\n",
       " 'he': 60,\n",
       " 'has': 58,\n",
       " 'rediscover': 107,\n",
       " 'his': 62,\n",
       " 'own': 96,\n",
       " 'capacity': 19,\n",
       " 'll': 75,\n",
       " 'take': 126,\n",
       " 'time': 136,\n",
       " 'republicans': 109,\n",
       " 'must': 84,\n",
       " 'hold': 63,\n",
       " 'primary': 102,\n",
       " 'under': 141,\n",
       " 'county': 25,\n",
       " 'unit': 142,\n",
       " 'system': 125,\n",
       " 'which': 150,\n",
       " 'party': 98,\n",
       " 'opposes': 95,\n",
       " 'its': 67,\n",
       " 'platform': 100,\n",
       " 'sam': 115,\n",
       " 'caldwell': 17,\n",
       " 'state': 123,\n",
       " 'highway': 61,\n",
       " 'department': 30,\n",
       " 'public': 104,\n",
       " 'relations': 108,\n",
       " 'director': 33,\n",
       " 'resigned': 111,\n",
       " 'tuesday': 140,\n",
       " 'work': 154,\n",
       " 'for': 46,\n",
       " 'lt': 77,\n",
       " 'gov': 55,\n",
       " 'garland': 51,\n",
       " 'byrd': 16,\n",
       " 'campaign': 18,\n",
       " 'resignation': 110,\n",
       " 'had': 57,\n",
       " 'been': 12,\n",
       " 'expected': 44,\n",
       " 'some': 120,\n",
       " 'be': 10,\n",
       " 'succeeded': 124,\n",
       " 'rob': 114,\n",
       " 'ledford': 70,\n",
       " 'gainesville': 49,\n",
       " 'who': 151,\n",
       " 'assistant': 6,\n",
       " 'more': 83,\n",
       " 'than': 127,\n",
       " 'three': 134,\n",
       " 'years': 156,\n",
       " 'mike': 81,\n",
       " 'liked': 73,\n",
       " 'having': 59,\n",
       " 'light': 72,\n",
       " 'up': 145,\n",
       " 'through': 135,\n",
       " 'ripples': 113,\n",
       " 'was': 148,\n",
       " 'goodness': 54,\n",
       " 'beauty': 11,\n",
       " 'they': 132,\n",
       " 'picnicked': 99,\n",
       " 'pool': 101,\n",
       " 'then': 130,\n",
       " 'lay': 69,\n",
       " 'back': 8,\n",
       " 'on': 91,\n",
       " 'grass': 56,\n",
       " 'looked': 76,\n",
       " 'stars': 122,\n",
       " 'there': 131,\n",
       " 'mars': 79}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "common-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVERSE MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "structured-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers=vc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "cross-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2s=cv.inverse_transform(vc[0])#THIS IS JUmbleD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "signed-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "looking-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2s=np.array(n2s).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "trying-coordination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'828 abstractions all an and are at bauer being budget by center cibula civic community continue cushion dallas dame davis designed di don ecsy evanston evelyn everett exhibited exhibition favor francisco franco gallery giovanni in it lammermoor leni line lucia mcnear month new next notre nov of open opera operatic paintings production reception returning san season semi st the this to trades university until use uses when will with zeffirelli'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(n2s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "cosmetic-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    " #custom tokenizer to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "crazy-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words=tokenizer.tokenize(document.lower())\n",
    "    words=remove_sw(words,sw)\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "impaired-devil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher school principal conferred everyone agreed kept certain amount work home little danger losing term .scotty accepted decision indifference enter arguments .he discharged hospital two day checkup parents mr. mckinley described celebration lunch cafeteria campus .rachel wore smart hat warned recently smoking puffed cigarettes long ivory holder stained lipstick .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(myTokenizer(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "adolescent-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv=CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "atlantic-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvc=ncv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "polyphonic-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "lightweight-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvc=nvc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "forty-adjustment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nvc[0])#Here we notice that earlier the length of each f vector was 159. Now it's 120 by removing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ceramic-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "specialized-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcv=CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "joint-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvc=bcv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "polish-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvc=bvc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "geographic-bahrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an exhibition': 5,\n",
       " 'exhibition of': 54,\n",
       " 'of evelyn': 109,\n",
       " 'evelyn cibula': 51,\n",
       " 'cibula paintings': 30,\n",
       " 'paintings will': 120,\n",
       " 'will open': 193,\n",
       " 'open with': 115,\n",
       " 'with reception': 195,\n",
       " 'reception nov': 129,\n",
       " 'nov at': 106,\n",
       " 'at the': 11,\n",
       " 'the evanston': 157,\n",
       " 'evanston community': 50,\n",
       " 'community center': 32,\n",
       " 'center 828': 29,\n",
       " '828 davis': 0,\n",
       " 'davis st': 39,\n",
       " 'st it': 145,\n",
       " 'it will': 84,\n",
       " 'will continue': 192,\n",
       " 'continue all': 33,\n",
       " 'all month': 3,\n",
       " 'month abstractions': 100,\n",
       " 'abstractions and': 1,\n",
       " 'and semi': 7,\n",
       " 'semi abstractions': 143,\n",
       " 'abstractions by': 2,\n",
       " 'by everett': 20,\n",
       " 'everett mcnear': 52,\n",
       " 'mcnear are': 97,\n",
       " 'are being': 8,\n",
       " 'being exhibited': 18,\n",
       " 'exhibited by': 53,\n",
       " 'by the': 23,\n",
       " 'the university': 165,\n",
       " 'university gallery': 182,\n",
       " 'gallery of': 63,\n",
       " 'of notre': 112,\n",
       " 'notre dame': 105,\n",
       " 'dame until': 38,\n",
       " 'until nov': 183,\n",
       " 'nov in': 107,\n",
       " 'in the': 81,\n",
       " 'the line': 160,\n",
       " 'line of': 92,\n",
       " 'of operatic': 113,\n",
       " 'operatic trades': 117,\n",
       " 'trades to': 177,\n",
       " 'to cushion': 173,\n",
       " 'cushion the': 35,\n",
       " 'the budget': 154,\n",
       " 'budget the': 19,\n",
       " 'the dallas': 156,\n",
       " 'dallas civic': 36,\n",
       " 'civic opera': 31,\n",
       " 'opera will': 116,\n",
       " 'will use': 194,\n",
       " 'use san': 185,\n",
       " 'san francisco': 139,\n",
       " 'francisco new': 59,\n",
       " 'new leni': 103,\n",
       " 'leni bauer': 89,\n",
       " 'bauer ecsy': 13,\n",
       " 'ecsy production': 48,\n",
       " 'production of': 126,\n",
       " 'of lucia': 111,\n",
       " 'lucia di': 96,\n",
       " 'di lammermoor': 42,\n",
       " 'lammermoor this': 86,\n",
       " 'this season': 169,\n",
       " 'season returning': 141,\n",
       " 'returning the': 135,\n",
       " 'the favor': 158,\n",
       " 'favor next': 56,\n",
       " 'next season': 104,\n",
       " 'season when': 142,\n",
       " 'when san': 188,\n",
       " 'francisco uses': 60,\n",
       " 'uses the': 186,\n",
       " 'dallas don': 37,\n",
       " 'don giovanni': 45,\n",
       " 'giovanni designed': 66,\n",
       " 'designed by': 41,\n",
       " 'by franco': 21,\n",
       " 'franco zeffirelli': 61,\n",
       " 'you don': 198,\n",
       " 'don eat': 44,\n",
       " 'eat enough': 47,\n",
       " 'enough honey': 49,\n",
       " 'honey try': 79,\n",
       " 'try to': 178,\n",
       " 'to get': 174,\n",
       " 'get that': 65,\n",
       " 'that down': 153,\n",
       " 'down rachel': 46,\n",
       " 'rachel observing': 128,\n",
       " 'observing would': 108,\n",
       " 'would say': 197,\n",
       " 'say he': 140,\n",
       " 'he has': 74,\n",
       " 'has to': 72,\n",
       " 'to rediscover': 175,\n",
       " 'rediscover his': 130,\n",
       " 'his own': 77,\n",
       " 'own capacity': 119,\n",
       " 'capacity it': 28,\n",
       " 'it ll': 82,\n",
       " 'll take': 93,\n",
       " 'take time': 151,\n",
       " 'the republicans': 163,\n",
       " 'republicans must': 132,\n",
       " 'must hold': 102,\n",
       " 'hold primary': 78,\n",
       " 'primary under': 125,\n",
       " 'under the': 180,\n",
       " 'the county': 155,\n",
       " 'county unit': 34,\n",
       " 'unit system': 181,\n",
       " 'system system': 149,\n",
       " 'system which': 150,\n",
       " 'which the': 189,\n",
       " 'the party': 161,\n",
       " 'party opposes': 121,\n",
       " 'opposes in': 118,\n",
       " 'in its': 80,\n",
       " 'its platform': 85,\n",
       " 'platform sam': 123,\n",
       " 'sam caldwell': 138,\n",
       " 'caldwell state': 26,\n",
       " 'state highway': 147,\n",
       " 'highway department': 76,\n",
       " 'department public': 40,\n",
       " 'public relations': 127,\n",
       " 'relations director': 131,\n",
       " 'director resigned': 43,\n",
       " 'resigned tuesday': 134,\n",
       " 'tuesday to': 179,\n",
       " 'to work': 176,\n",
       " 'work for': 196,\n",
       " 'for lt': 57,\n",
       " 'lt gov': 95,\n",
       " 'gov garland': 68,\n",
       " 'garland byrd': 64,\n",
       " 'byrd campaign': 24,\n",
       " 'campaign caldwell': 27,\n",
       " 'caldwell resignation': 25,\n",
       " 'resignation had': 133,\n",
       " 'had been': 70,\n",
       " 'been expected': 17,\n",
       " 'expected for': 55,\n",
       " 'for some': 58,\n",
       " 'some time': 144,\n",
       " 'time he': 172,\n",
       " 'he will': 75,\n",
       " 'will be': 191,\n",
       " 'be succeeded': 14,\n",
       " 'succeeded by': 148,\n",
       " 'by rob': 22,\n",
       " 'rob ledford': 137,\n",
       " 'ledford of': 88,\n",
       " 'of gainesville': 110,\n",
       " 'gainesville who': 62,\n",
       " 'who has': 190,\n",
       " 'has been': 71,\n",
       " 'been an': 16,\n",
       " 'an assistant': 4,\n",
       " 'assistant more': 9,\n",
       " 'more than': 101,\n",
       " 'than three': 152,\n",
       " 'three years': 170,\n",
       " 'mike liked': 98,\n",
       " 'liked having': 91,\n",
       " 'having light': 73,\n",
       " 'light up': 90,\n",
       " 'up through': 184,\n",
       " 'through the': 171,\n",
       " 'the ripples': 164,\n",
       " 'ripples it': 136,\n",
       " 'it was': 83,\n",
       " 'was goodness': 187,\n",
       " 'goodness beauty': 67,\n",
       " 'beauty they': 15,\n",
       " 'they picnicked': 168,\n",
       " 'picnicked by': 122,\n",
       " 'the pool': 162,\n",
       " 'pool then': 124,\n",
       " 'then lay': 166,\n",
       " 'lay back': 87,\n",
       " 'back on': 12,\n",
       " 'on the': 114,\n",
       " 'the grass': 159,\n",
       " 'grass and': 69,\n",
       " 'and looked': 6,\n",
       " 'looked at': 94,\n",
       " 'at stars': 10,\n",
       " 'stars mike': 146,\n",
       " 'mike there': 99,\n",
       " 'there mars': 167}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "posted-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramcv=CountVectorizer(ngram_range=(1,3))#all sentences will be in 1,2,3,4 clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "burning-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"This is good movie\"\n",
    "sent2=\"This was good movie\"\n",
    "sent3=\"This is not good movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "historic-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2=[sent1,sent2,sent3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "eligible-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "hollywood-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "mounted-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "toxic-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvc=tfidf.fit_transform(corpus2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "filled-nebraska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46333427, 0.59662724, 0.46333427, 0.        , 0.46333427,\n",
       "        0.        ],\n",
       "       [0.41285857, 0.        , 0.41285857, 0.        , 0.41285857,\n",
       "        0.69903033],\n",
       "       [0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvc#As we can see it gives importance of each word closer to 0 means not very useful and unique and opp for closer to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cooperative-difficulty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-vacation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
